// Copyright (c) 2003-2006 King's College London, created by Laurence Tratt
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to
// deal in the Software without restriction, including without limitation the
// rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
// sell copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
// FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
// IN THE SOFTWARE.


import Array, Builtins, Exceptions, Numbers, PCRE, Strings, Sys
import CPK::Token, CPK::Tokens, Parser




_RE_WHITESPACE := PCRE::compile("[ \t\n\r]+")
_RE_ID := PCRE::compile("[a-zA-Z_][a-zA-Z_0-9]*")
_RE_SYMBOLS := PCRE::compile("(::=|%precedence|%long|%short|[(]|[)][*]|[)][?]|[)][+]|\\|)")
_RE_TOKEN := PCRE::compile("\"(.*?)\"")
_RE_INT := PCRE::compile("[0-9]+")



/////////////////////////////////////////////////////////////////////////////////////////////////////
// The grammar
//
// In an ideal world we would write this as follows:
//
//   _GRAMMAR := """..."""
//   $c<[| &_TOKENS_MAP, &_BOOTSTRAPPED_GRAMMAR, &_BOOTSTRAPPED_RULE_NAMES := compile(...) |]>
//
// However introducing CTMP into a part of the compiler such as this is simply more trouble than it's
// worth. Therefore we "manually" bootstrap the below grammar with the _BOOTSTRAP_* variables; in
// essence we've done a splice "by hand". What this means is that if (for some reason) you need to
// change the grammar in _GRAMMAR (unlikely, but not impossible), you will need to re-do this manual
// process. If you don't do that, nothing will appear to have changed.
//

_GRAMMAR := """
grammar    ::= ( rule )*

rule       ::= "NONTERM" "::=" alt ( "|" alt )*
alt        ::= symbols precedence
             | precedence

symbols    ::= ( symbol )+
symbol     ::= "NONTERM"
             | "TERM"
             | "(" symbols ")*"
             | "(" symbols ")+"
             | "(" symbols ")?"

precedence ::= "%PRECEDENCE" "INT"
             |
"""

_BOOTSTRAPPED_TOKENS_MAP := Dict{"|" : 8, "?" : 4, "NONTERM" : 0, "(" : 9, ")" : 10, "*" : 2, "+" : 3, "INT" : 7, ")?" : 13, "::=" : 5, "%PRECEDENCE" : 6, ")*" : 11, ")+" : 12, "TERM" : 1}

_BOOTSTRAPPED_GRAMMAR := [5, 120, 155, 180, -1, 14, 19, 24, 33, 50, 57, 62, 73, 78, 83, 92, 101, 110, 117, 0, 0, 2, 0, 1, 0, 1, 6, 2, 0, 0, 2, 3, 1, 0, 2, 14, 1, 0, 1, 5, 0, 3, 2, 4, 1, 8, 0, 3, 3, 5, 0, 3, 4, 0, 4, 0, 6, 0, 3, 2, 0, 6, 0, 4, 8, 0, 5, 2, 2, 0, 5, 3, 3, 0, 5, 2, 1, 0, 0, 5, 2, 1, 1, 0, 5, 6, 1, 9, 0, 4, 1, 11, 0, 5, 6, 1, 9, 0, 4, 1, 12, 0, 5, 6, 1, 9, 0, 4, 1, 13, 0, 6, 4, 1, 6, 1, 7, 0, 6, 0, 7, 128, 131, 134, 137, 141, 144, 151, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 2, 3, 4, 0, 1, 5, 0, 5, 6, 7, 8, 9, 10, 1, 2, 11, 12, 6, 162, 165, 168, 171, 174, 177, 2, 2, 6, 2, 6, 2, 2, 4, 8, 2, 8, 4, 2, 8, 14, 2, 14, 8, 6, 187, 190, 193, 196, 199, 202, 2, 2, -2, 2, -2, 2, 2, 4, 0, 2, 0, 4, 2, 10, 4, 2, 4, 10]

_BOOTSTRAPPED_RULE_NAMES := ["__start__", "grammar", "rule", "alt", "symbols", "symbol", "precedence"]



////////////////////////////////////////////////////////////////////////////////////////////////////
//
//

_COMPILED_GRAMMAR_RULES_OFFSET := 0
_COMPILED_GRAMMAR_RULES_ALTERNATIVES := 1
_COMPILED_GRAMMAR_RECOGNISER_BRACKETS_MAPS_OFFSETS := 2
_COMPILED_GRAMMAR_PARSER_BRACKETS_MAPS_OFFSETS := 3

_RULE_PRECEDENCE := 0
_RULE_PARENT_RULE := 1
_RULE_NUM_SYMBOLS := 2
_RULE_SYMBOLS := 3

_COMPILED_RULE_REF := 0
_COMPILED_TOKEN := 1
_COMPILED_OPEN_KLEENE_STAR_GROUP := 2
_COMPILED_CLOSE_KLEENE_STAR_GROUP := 3
_COMPILED_OPEN_OPTIONAL_GROUP := 4
_COMPILED_CLOSE_OPTIONAL_GROUP := 5




class _Parser:

    func parse(self, grammar, start_rule, tokens_map):

        self._start_rule := start_rule
        self._tokens_map := tokens_map

        tokens_map := _BOOTSTRAPPED_TOKENS_MAP
        comp_grammar := Array::Array.new("i")
        comp_grammar.extend(_BOOTSTRAPPED_GRAMMAR)
        comp_grammar := comp_grammar.serialize()
        rule_names := _BOOTSTRAPPED_RULE_NAMES

        tokens, newlines := self._tokenize(grammar)
        parser := Parser::Parser.new()
        pt := parser.parse(comp_grammar, rule_names, tokens_map, tokens)
        rules := self._t_grammar(pt)

        return [self._rule_names_map, self._ordered_rule_names, rules]



    func _tokenize(self, grammar):

        i := 0
        tokens := []
        newlines := []
        while i < grammar.len():
            if m := _RE_WHITESPACE.match(grammar, i):
                i += m[0].len()
                continue

            if m := _RE_ID.match(grammar, i):
                tokens.append(Token::Token.new("NONTERM", m[0], [[null, i]]))
                i += m[0].len()
                continue

            if m := _RE_TOKEN.match(grammar, i):
                tokens.append(Token::Token.new("TERM", m[1], [[null, i]]))
                i += m[0].len()
                continue

            if m := _RE_SYMBOLS.match(grammar, i):
                tokens.append(Token::Token.new(m[0].upper_cased(), m[0], [[null, i]]))
                i += m[0].len()
                continue

            if m := _RE_INT.match(grammar, i):
                tokens.append(Token::Token.new("INT", m[0], [[null, i]]))
                i += m[0].len()
                continue

            Sys::println(i, grammar[i : ])		
            raise "XXX"

        if tokens.len() > 0 & tokens[0].type == "NEWLINE":
            tokens.del(0)

        if tokens.len() > 0 & tokens[-1].type == "NEWLINE":
            tokens.del(-1)

        return [tokens, newlines]



    func _error(self, msg):

        raise Exceptions::User_Exception.new(msg)



    func _t_grammar(self, node):

        // grammar ::= ( rule )*

        self._ordered_rule_names := ["__start__"]
        self._rule_names_map := Dict{"__start__" : 0}
        for rule_node := node.iter(1):
            rule_name := rule_node[1].value
            if self._rule_names_map.find(rule_name):
                self._error(Strings::format("Duplicate rule name '%s'.", rule_name))
            self._ordered_rule_names.append(rule_name)
            self._rule_names_map[rule_name] := self._rule_names_map.len()

        start_alt := [0, 0, 2, _COMPILED_RULE_REF, self._rule_names_map[self._start_rule]]
        rules := Dict{0 : [start_alt]}
        for rule_node := node.iter(1):
            name, alts := self._t_rule(rule_node)
            mapped_name := self._rule_names_map[name]
            rules[mapped_name] := alts

        return rules



    func _t_rule(self, node):

        // rule ::= "NONTERM" "::=" alt ( "|" alt )*

        name := node[1].value
        alts := []
        rule_num := self._rule_names_map[name]
        i := 3
        while i < node.len():
            alts.append(self._t_alt(node[i], rule_num))
            i += 2

        return [name, alts]



    func _t_alt(self, node, rule_num):

        // alt ::= symbols precedence
        //       | precedence

        prec_node := node[-1]
        if prec_node.len() == 1:
            // precedence ::=
            prec := 0
        else:
            // precedence ::= "%PRECEDENCE" "INT"
            prec := Builtins::Int.new(prec_node[2].value)

        if node.len() == 1:
            // alt ::=
            return [prec, rule_num, 0]

        symbols := self._t_symbols(node[1])

        return [prec, rule_num, symbols.len()] + symbols



    func _t_symbols(self, node):

        // symbols ::= ( symbol )*

        symbols := []
        for symbol_node := node.iter(1):
            snt := symbol_node[1].type
            if snt == "NONTERM":
                // symbol ::= "NONTERM"
                symbols.extend([_COMPILED_RULE_REF, self._rule_names_map[symbol_node[1].value]])
            elif snt == "TERM":
                // symbol ::= "TERM"
                symbols.extend([_COMPILED_TOKEN, self._tokens_map[symbol_node[1].value]])
            else:
                // symbol ::= "(" symbols ")*"
                //        ::= "(" symbols ")+"
                //        ::= "(" symbols ")?"
                sub_symbols := self._t_symbols(symbol_node[2])
                if symbol_node[3].type == ")*":
                    symbols.extend([_COMPILED_OPEN_KLEENE_STAR_GROUP, -1])
                    symbols.extend(sub_symbols)
                    symbols.extend([_COMPILED_CLOSE_KLEENE_STAR_GROUP, -1])
                elif symbol_node[3].type == ")+":
                    symbols.extend(sub_symbols)
                    symbols.extend([_COMPILED_OPEN_KLEENE_STAR_GROUP, -1])
                    symbols.extend(sub_symbols)
                    symbols.extend([_COMPILED_CLOSE_KLEENE_STAR_GROUP, -1])
                elif symbol_node[3].type == ")+":
                    symbols.extend([_COMPILED_OPEN_OPTIONAL_GROUP, -1])
                    symbols.extend(sub_symbols)
                    symbols.extend([_COMPILED_CLOSE_OPTIONAL_GROUP, -1])

        return symbols



func _mk_nullables(rules):

    // The nullable computation is loosely based on that found in John Aycock's SPARK. It's a two
    // step process.
    //
    // First, iterate through and find every rule which has an alternative which is definitely
    // nullable. All other rules are added to 'tbd' and will be considered in the second step.

    nullables := Set{}
    tbd := Set{}
    for mapped_name, alts := rules.iter():
        for alt := alts.iter():
            bc := 0
            for i := _RULE_SYMBOLS.iter_to(alt.len(), 2):
                ndif alt[i] == _COMPILED_OPEN_KLEENE_STAR_GROUP | alt[i] == _COMPILED_OPEN_OPTIONAL_GROUP:
                    bc += 1
                elif alt[i] == _COMPILED_CLOSE_KLEENE_STAR_GROUP | alt[i] == _COMPILED_CLOSE_OPTIONAL_GROUP:
                    bc -= 1
                elif alt[i] == _COMPILED_RULE_REF:
                    if bc == 0:
                        break
                elif alt[i] == _COMPILED_TOKEN:
                    if bc == 0:
                        break
            exhausted:
                nullables.add(mapped_name)
                break
        exhausted:
            tbd.add(mapped_name)

    // Second, continuously iterate over every "to be decided" rule finding if the growing set of
    // known nullable rules makes a rule nullable.

    while 1:
        changed := 0
        for mapped_name := tbd.iter():
            if nullables.find(mapped_name):
                // We don't remove entries from tbd, as that's too fiddly; so if a rule is now
                // known to be nullable, we simply don't consider it.
                continue

            for alt := rules[mapped_name].iter():
                bc := 0
                for i := _RULE_SYMBOLS.iter_to(alt.len(), 2):
                    ndif alt[i] == _COMPILED_OPEN_KLEENE_STAR_GROUP | alt[i] == _COMPILED_OPEN_OPTIONAL_GROUP:
                        bc += 1
                    elif alt[i] == _COMPILED_CLOSE_KLEENE_STAR_GROUP | alt[i] == _COMPILED_CLOSE_OPTIONAL_GROUP:
                        bc -= 1
                    elif alt[i] == _COMPILED_RULE_REF:
                        if bc == 0 & not nullables.find(alt[i + 1]):
                            break
                    elif alt[i] == _COMPILED_TOKEN:
                        if bc == 0:
                            break
                exhausted:
                    nullables.add(mapped_name)
                    changed := 1
                    break
        if changed == 0:
            break

    return nullables



func _mk_brackets_maps(rules):

    recogniser_brks_map := []
    parser_brks_map := []
    for alt := rules.iter_vals().iter():
        for i := _RULE_SYMBOLS.iter_to(alt.len(), 2):
            type := alt[i]
            if type == _COMPILED_OPEN_KLEENE_STAR_GROUP | type == _COMPILED_CLOSE_KLEENE_STAR_GROUP \
              | type == _COMPILED_OPEN_OPTIONAL_GROUP | type == _COMPILED_CLOSE_OPTIONAL_GROUP:
                alt[i + 1] := recogniser_brks_map.len()
                recogniser_brks_map.append(_mk_recogniser_brks_map(alt, i))
                parser_brks_map.append(_mk_parser_brks_map(alt, i))

            i += 2

    return [recogniser_brks_map, parser_brks_map]



func _mk_recogniser_brks_map(alt, j):

    if j < alt.len() & [_COMPILED_OPEN_KLEENE_STAR_GROUP, _COMPILED_CLOSE_KLEENE_STAR_GROUP, _COMPILED_OPEN_OPTIONAL_GROUP, _COMPILED_CLOSE_OPTIONAL_GROUP].find(alt[j]):
        bracket_map := _mk_recogniser_brks_map(alt, j + 2)
        if alt[j] == _COMPILED_OPEN_KLEENE_STAR_GROUP | alt[j] == _COMPILED_OPEN_OPTIONAL_GROUP:
            count := 1
            j += 2
            while count > 0:
                if alt[j] == _COMPILED_CLOSE_OPTIONAL_GROUP | alt[j] == _COMPILED_CLOSE_KLEENE_STAR_GROUP:
                    count -= 1
                j += 2
        elif alt[j] == _COMPILED_CLOSE_KLEENE_STAR_GROUP:
            count := 1
            j -= 2
            while count > 0:
                if alt[j] == _COMPILED_OPEN_KLEENE_STAR_GROUP:
                    count -= 1
                elif alt[j] == _COMPILED_CLOSE_KLEENE_STAR_GROUP:
                    count += 1
                j -= 2
            j += 4
        elif alt[j] == _COMPILED_CLOSE_OPTIONAL_GROUP:
            return bracket_map
        else:
            raise "XXX"
        for new_j := _mk_recogniser_brks_map(alt, j).iter():
            if not bracket_map.find(new_j):
                bracket_map.append(new_j)
        return bracket_map
    else:
        return [j - _RULE_SYMBOLS]




func _mk_parser_brks_map( alt, j):

    if j < alt.len() & (alt[j] == _COMPILED_OPEN_KLEENE_STAR_GROUP | alt[j] == _COMPILED_CLOSE_KLEENE_STAR_GROUP | alt[j] == _COMPILED_OPEN_OPTIONAL_GROUP | alt[j] == _COMPILED_CLOSE_OPTIONAL_GROUP):
        if alt[j] == _COMPILED_OPEN_KLEENE_STAR_GROUP:
            count := 1
            k := j + 2
            while count > 0:
                if alt[k] == _COMPILED_OPEN_KLEENE_STAR_GROUP:
                    count += 1
                elif alt[k] == _COMPILED_CLOSE_KLEENE_STAR_GROUP:
                    count -= 1
                k += 2
            brackets_map := _mk_parser_brks_map(alt, k - 4)
            if j - _RULE_SYMBOLS == 0:
                brackets_map.append(-2)
            else:
                brackets_map.extend(_mk_parser_brks_map(alt, j - 2))

            return brackets_map
        elif alt[j] == _COMPILED_CLOSE_KLEENE_STAR_GROUP:
            count := 1
            k := j - 2
            while count > 0:
                if alt[k] == _COMPILED_OPEN_KLEENE_STAR_GROUP:
                    count -= 1
                elif alt[k] == _COMPILED_CLOSE_KLEENE_STAR_GROUP:
                    count += 1
                k -= 2

            if k - _RULE_SYMBOLS >= 0:
                brackets_map := _mk_parser_brks_map(alt, k)
            else:
                brackets_map := [-2]
            brackets_map.extend(_mk_parser_brks_map(alt, j - 2))

            return brackets_map
        elif alt[j] == _COMPILED_OPEN_OPTIONAL_GROUP:
            if j - _RULE_SYMBOLS == 0:
                return [-2]
            else:
                return _mk_parser_brks_map(alt, j - 2)
        else:
            count := 1
            k := j - 2
            while count > 0:
                if alt[k] == _COMPILED_OPEN_OPTIONAL_GROUP:
                    count -= 1
                elif alt[k] == _COMPILED_CLOSE_OPTIONAL_GROUP:
                    count += 1
                k -= 2

            if k - _RULE_SYMBOLS >= 0:
                brackets_map := _mk_parser_brks_map(alt, k)
            else:
                brackets_map := [-2]
            brackets_map.extend(_mk_parser_brks_map(alt, j - 2))

            return brackets_map
    else:
        return [j - _RULE_SYMBOLS]



func compile(grammar, start_rule, tokens_map):

    rule_names_map, ordered_rule_names, rules := _Parser.new().parse(grammar, start_rule, tokens_map)
    nullables := _mk_nullables(rules)
    recogniser_brks_map, parser_brks_map := _mk_brackets_maps(rules)

    total_alts := 0
    for alts := rules.iter_vals():
        total_alts += alts.len()

    comp := Array::Array.new("i")
    comp.append(-1) // Offset of all rules
    comp.append(-1) // Offset of rules / alternatives
    comp.append(-1) // Offset of rule names
    comp.append(-1) // Offset of recogniser brackets maps
    comp.append(-1) // Offset of parser brackets maps

    comp[_COMPILED_GRAMMAR_RULES_OFFSET] := comp.len()
    comp.append(total_alts + 1)
    comp_alts_offsets := comp.len()
    comp.extend([-1] * total_alts)

    i := 0
    for mapped_name := 0.iter_to(rule_names_map.len()):
        for alt := rules[mapped_name].iter():
            comp[comp_alts_offsets + i] := comp.len()
            comp.extend(alt)
            i += 1

    comp[_COMPILED_GRAMMAR_RULES_ALTERNATIVES] := comp.len()
    comp.append(rule_names_map.len())
    comp_alts_map := comp.len()
    comp.extend([-1] * rule_names_map.len())
    i := 0
    for mapped_name := 0.iter_to(rule_names_map.len()):
        comp[comp_alts_map + mapped_name] := comp.len()
        if nullables.find(mapped_name):
            comp.append(1)
        else:
            comp.append(0)
        alts := rules[mapped_name]
        comp.append(alts.len())
        for alt := alts.iter():
            comp.append(i)
            i += 1

    comp[_COMPILED_GRAMMAR_RECOGNISER_BRACKETS_MAPS_OFFSETS] := comp.len()
    comp.append(recogniser_brks_map.len())
    comp_recogniser_brks_map := comp.len()
    comp.extend([-1] * recogniser_brks_map.len())
    for i := 0.iter_to(recogniser_brks_map.len()):
        comp[comp_recogniser_brks_map + i] := comp.len()
        comp.append(recogniser_brks_map[i].len())
        comp.extend(recogniser_brks_map[i])

    comp[_COMPILED_GRAMMAR_PARSER_BRACKETS_MAPS_OFFSETS] := comp.len()
    comp.append(parser_brks_map.len())
    comp_parser_brks_map := comp.len()
    comp.extend([-1] * parser_brks_map.len())
    for i := 0.iter_to(parser_brks_map.len()):
        comp[comp_parser_brks_map + i] := comp.len()
        comp.append(parser_brks_map[i].len())
        comp.extend(parser_brks_map[i])

    return [comp.serialize(), ordered_rule_names]
